---
title: "SAall"
author: "JMitic01"
date: "2024-08-01"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
---


```{r plot-data}
library(MASS)
library(mclust)
library(ggplot2)
library(dplyr)
library(purrr)
library(knitr)
library(reshape2)
library(gridExtra)

big_data <- read.table("C:/Users/jovan/Downloads/FallData_all.txt", header = TRUE)
# Define the pattern for subjects SA01 to SA23
pattern <- "^SA(0[1-9]|1[0-9]|2[0-3])$"
subject <- subset(big_data, grepl(pattern, big_data$Subject))
selected_variables <- c("max", "min", "max_diff")
subject_data <- subset(big_data, Subject == subject)
selected_data <- subject_data[, selected_variables]
output_directory <- 'C:/Users/jovan/OneDrive/Desktop/Stat Reading Group'

# Create ggplot2 plots
plot1 <- ggplot(selected_data, aes(x = max, y = min)) +
  geom_point(size = 2) +
  labs(title = "Max vs Min",
       x = "Max",
       y = "Min") +
  theme_minimal()

plot2 <- ggplot(selected_data, aes(x = min, y = max_diff)) +
  geom_point(size = 2) +
  labs(title = "Min vs Max_diff",
       x = "Min",
       y = "Max_diff") +
  theme_minimal()

plot3 <- ggplot(selected_data, aes(x = max, y = max_diff)) +
  geom_point(size = 2) +
  labs(title = "Max vs Max_diff",
       x = "Max",
       y = "Max_diff") +
  theme_minimal()

# Display plots
grid.arrange(plot1, plot2, plot3, ncol = 3)

# Define a function to create plots for a specific subject
create_plots_for_subject <- function(subject_data, subject_name) {
  selected_data <- subject_data[, selected_variables]

  plot1 <- ggplot(selected_data, aes(x = max, y = min)) +
    geom_point(size = 2) +
    labs(title = paste("Data for subject", subject_name),
         x = "Max",
         y = "Min") +
    theme_minimal()

  plot2 <- ggplot(selected_data, aes(x = min, y = max_diff)) +
    geom_point(size = 2) +
    labs(title = paste("Data for subject", subject_name),
         x = "Min",
         y = "Max_diff") +
    theme_minimal()

  plot3 <- ggplot(selected_data, aes(x = max, y = max_diff)) +
    geom_point(size = 2) +
    labs(title = paste("Data for subject", subject_name),
         x = "Max",
         y = "Max_diff") +
    theme_minimal()

  grid.arrange(plot1, plot2, plot3, ncol = 3)
}

# Loop through each subject
unique_subjects <- unique(subject_data$Subject)
for (subject in unique_subjects) {
  subject_subset <- subset(subject_data, Subject == subject)
  create_plots_for_subject(subject_subset, subject)
}

```

```{r best-model-selection}
G_values <- 1:10
model_names <- c("EVI", "EEI", "EEV", "VVV", "EII")
best_bic <- -Inf
best_G <- NULL
best_model_name <- NULL

for (model_name in model_names) {
  for (G in G_values) {
    result <- tryCatch(Mclust(selected_data, G = G, modelName = model_name), error = function(e) NULL)
    if (!is.null(result) && !is.null(result$BIC) && result$BIC > best_bic) {
      best_bic <- result$BIC
      best_G <- G
      best_model_name <- model_name
    }
  }
}

best_G
best_model_name

```
In the analysis of the data for all SA01-SA23 subjects data, we aimed to identify the best clustering model using the Mclust algorithm, which fits Gaussian finite mixture models to the data. The model selection was based on the Bayesian Information Criterion (BIC) values obtained for different combinations of model types and the number of clusters (G).

After evaluating various models and cluster counts, the model with the highest BIC value was identified as the VVV model with 9 clusters. VVV Model Description: Variable Volume: Each cluster can have a different volume. This means that the determinant of the covariance matrix can vary between clusters.
Variable Shape: Each cluster can have a different shape. The eigenvalues of the covariance matrices (after accounting for volume) can vary between clusters.
Variable Orientation: The orientation of each cluster can differ. This is reflected in the eigenvectors of the covariance matrices, which can be different for each cluster.//
```{r plot-best-clustering}
result <- Mclust(selected_data, G = 9, modelName = "VVV")
cluster_labels <- result$classification


plot1 <- ggplot(selected_data, aes(x = max, y = min, color = as.factor(cluster_labels))) +
  geom_point(size = 2) +  # Increase point size for better visibility
  scale_color_discrete(name = "Cluster Labels") +
  labs(title = "Max vs Min") +
  theme_minimal() +
  guides(shape = guide_legend(override.aes = list(size = 2)))  # Adjust legend size

plot2 <- ggplot(selected_data, aes(x = min, y = max_diff, color = as.factor(cluster_labels))) +
  geom_point(size = 2) +  # Increase point size for better visibility
  scale_color_discrete(name = "Cluster Labels") +
  labs(title = "Min vs Max_diff") +
  theme_minimal() +
  guides(shape = guide_legend(override.aes = list(size = 2)))  # Adjust legend size

plot3 <- ggplot(selected_data, aes(x = max, y = max_diff, color = as.factor(cluster_labels))) +
  geom_point(size = 2) +  # Increase point size for better visibility
  scale_color_discrete(name = "Cluster Labels") +
  labs(title = "Max vs Max_diff") +
  theme_minimal() +
  guides(shape = guide_legend(override.aes = list(size = 2)))
print(plot3)
print(plot2)
print(plot1)
```
```{r uncertainty_plots}
par(mfrow=c(1,2), cex=0.7, mar=c(3.1,4.1,1,0.5), mgp=c(1.8,0.5,0), bty="L", oma=c(0,0,1,0))


# Uncertainty plot
plot(result, what = "uncertainty")

# Classification plot with ellipses
plot(result, what = "classification")

```
The uncertainty Plot provides a visual representation of the uncertainty in the assignment of each data point to the identified clusters. Helps in understanding the spread and orientation of clusters using ellipses. Points with lighter colors are more uncertain in their cluster assignment, while darker points are more confidently assigned. High uncertainty in certain areas might suggest the presence of overlapping clusters or regions where the data does not distinctly separate into the specified number of clusters.\\
The classification plot displays the data points color-coded by their assigned cluster, with ellipses representing the estimated boundaries of each cluster. These ellipses are derived from the Gaussian mixture model used in the clustering process, showing the spread and orientation of each cluster. Points within the same ellipse are more similar to each other than to points outside.

```{r ari_results}
calculate_ari <- function(n_fraction) {
  n <- nrow(selected_data)
  n_subsample <- floor(n * n_fraction)
  
  ari_values <- numeric(20)
  
  for (i in 1:20) {
    subset_indices <- sample(1:n, n_subsample, replace = FALSE)
    subset_data <- selected_data[subset_indices, ]
    subset_result <- tryCatch(Mclust(subset_data, G = best_G, modelName = best_model_name), error = function(e) NULL)
    
    if (!is.null(subset_result) && !is.null(subset_result$classification)) {
      subset_classification <- subset_result$classification
      ari_values[i] <- adjustedRandIndex(result$classification[subset_indices], subset_classification)
    } else {
      ari_values[i] <- NA
    }
  }
  
  return(ari_values)
}

n_values <- c(3/4, 1/2, 1/4)
ari_results <- map_df(n_values, function(n) {
  ari <- calculate_ari(n)
  data.frame(n_fraction = rep(n, length(ari)), ARI = ari)
})

ari_results <- ari_results %>% filter(!is.na(ARI))
#print(ari_results)
```
The stability analysis conducted above provide insights into the robustness of the clustering algorithm across different subsample fractions (n_fraction) for a given dataset. The Adjusted Rand Index (ARI) values computed represent the degree of similarity between the clustering results obtained from subsets of the data.\\
The Adjusted Rand Index (ARI) results provide insight into the stability and consistency of clustering as the size of the data fraction (n_fraction) changes. When n=0.75, the ARI values range from 0.5256939 to 1.0000000, with most values clustering around higher scores, indicating a generally stable clustering performance with some variability. When the data fraction is reduced to 0.50 the ARI scores range from 0.4307185 to 1.0000000, suggesting slightly less stability. Finally, at n=0.25, the ARI ranges from 0.5289797 to 1.0000000, and show a decrease in consistency compared to higher data fractions. This pattern indicates that while high ARI scores can still be achieved with smaller data fractions, the clustering performance becomes less stable and more sensitive to data size reduction.

```{r plot-ari}

# Calculate the average ARI for each n_fraction
average_ari <- ari_results %>%
  group_by(n_fraction) %>%
  summarize(avg_ari = mean(ARI))

# Plot ARI values against n values
ggplot(ari_results, aes(x = as.factor(n_fraction), y = ARI)) +
  geom_point(color = "skyblue", size = 3) +
  geom_point(data = average_ari, aes(y = avg_ari), color = "red", size = 4, shape = 18) +
  labs(x = "n Fraction", y = "ARI", title = "ARI Values for G = 2") +
  theme_minimal()




```

Now, we will perform the same  analysis but for G=3 (EEV), G=4 (VVV), and G=5 (EEV).
```{r ari_moreGvalues}
find_best_model <- function(data, G) {
  model_names <- c("EVI", "EEI", "EEV", "VVV", "EII")
  best_bic <- -Inf
  best_model_name <- NULL

  for (model_name in model_names) {
    result <- tryCatch(Mclust(data, G = G, modelName = model_name), error = function(e) NULL)
    if (!is.null(result) && !is.null(result$BIC) && result$BIC > best_bic) {
      best_bic <- result$BIC
      best_model_name <- model_name
    }
  }
  
  return(best_model_name)
}
G_values <- 1:12
best_models <- list()

for (G in G_values) {
  best_model_name <- find_best_model(selected_data, G)
  best_models[[as.character(G)]] <- best_model_name
}

# Print the best models for each G
print(best_models)
calculate_ari <- function(n_fraction, G, best_model_name) {
  n <- nrow(selected_data)
  n_subsample <- floor(n * n_fraction)
  
  ari_values <- numeric(20)
  
  for (i in 1:20) {
    subset_indices <- sample(1:n, n_subsample, replace = FALSE)
    subset_data <- selected_data[subset_indices, ]
    subset_result <- tryCatch(Mclust(subset_data, G = G, modelName = best_model_name), error = function(e) NULL)
    
    if (!is.null(subset_result) && !is.null(subset_result$classification)) {
      subset_classification <- subset_result$classification
      ari_values[i] <- adjustedRandIndex(result$classification[subset_indices], subset_classification)
    } else {
      ari_values[i] <- NA
    }
  }
  
  return(ari_values)
}

# Perform ARI stability analysis for different G values and subsample fractions
n_values <- c(3/4, 1/2, 1/4)
ari_results_list <- list()

for (G in G_values) {
  best_model_name <- best_models[[as.character(G)]]
  ari_results <- map_df(n_values, function(n) {
    ari <- calculate_ari(n, G, best_model_name)
    data.frame(n_fraction = rep(n, length(ari)), G = rep(G, length(ari)), ARI = ari)
  })
  ari_results_list[[as.character(G)]] <- ari_results %>% filter(!is.na(ARI))
}

# Combine results from all G values
final_ari_results <- bind_rows(ari_results_list)

# Filter ARI results for each G value
ari_results_G1 <- final_ari_results %>% filter(G == 1)
ari_results_G2 <- final_ari_results %>% filter(G == 2)
ari_results_G3 <- final_ari_results %>% filter(G == 3)
ari_results_G4 <- final_ari_results %>% filter(G == 4)
ari_results_G5 <- final_ari_results %>% filter(G == 5)
ari_results_G6 <- final_ari_results %>% filter(G == 6)
ari_results_G7 <- final_ari_results %>% filter(G == 7)
ari_results_G8 <- final_ari_results %>% filter(G == 8)
ari_results_G9 <- final_ari_results %>% filter(G == 9)
ari_results_G10 <- final_ari_results %>% filter(G == 10)
ari_results_G11 <- final_ari_results %>% filter(G == 11)
ari_results_G12 <- final_ari_results %>% filter(G == 12)




# Print the ARI results for each G value
#print("ARI results for G = 3:")
#print(ari_results_G3)

#print("ARI results for G = 4:")
#print(ari_results_G4)

#print("ARI results for G = 5:")
#print(ari_results_G5)

# Plot the ARI results
ggplot(final_ari_results, aes(x = as.factor(G), y = ARI, fill = as.factor(n_fraction))) +
  geom_boxplot() +
  labs(title = "ARI Stability Analysis for Different G Values",
       x = "Number of Components (G)",
       y = "Adjusted Rand Index (ARI)",
       fill = "Subsample Fraction") +
  theme_minimal()
```

`